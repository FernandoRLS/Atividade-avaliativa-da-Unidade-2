Trabalho em Grupo: Impacto da Evolução Computacional

Grupo: *inserir nomes*

Tema: Análise de Grandes Volumes de Dados para Previsão de Tendências de Mercado

2. Pesquisa e Análise da Evolução Computacional Aplicada ao Tema

Linha do Tempo Detalhada:

Década de 1950-60:

Uso de métodos estatísticos clássicos como regressão linear, que envolviam cálculos manuais ou com computadores limitados, como o IBM 650, utilizado principalmente em áreas acadêmicas e governamentais (Draper e Smith, 1966).

Década de 1970-80:

Edgar F. Codd introduziu o conceito dos bancos de dados relacionais, revolucionando a maneira como dados eram armazenados e consultados (Codd, 1970).

Primeiros sistemas de gestão de bancos de dados (DBMS), como o Oracle Database lançado em 1979, melhoraram significativamente a eficiência e escalabilidade da análise de dados empresariais.

Década de 1990:

Popularização do conceito de data warehouses por Bill Inmon (1992) e Ralph Kimball (1996), permitindo análises integradas e complexas de grandes volumes de dados históricos.

Crescimento do uso de técnicas avançadas de mineração de dados e descoberta de conhecimento (Fayyad et al., 1996).

Década de 2000:

Surgimento do conceito "Big Data", definido por Doug Laney em 2001, destacando os desafios de volume, velocidade e variedade dos dados.

Adoção massiva da computação em nuvem para armazenamento e processamento de grandes volumes de dados, representada por serviços pioneiros como Amazon Web Services (AWS), lançados em 2006.

Avanços no aprendizado de máquina com algoritmos como Random Forests, proposto por Leo Breiman (2001), que permitiu maior precisão nas previsões de mercado.

Década de 2010 até hoje:

Revolução do deep learning através dos trabalhos de Geoffrey Hinton, Yoshua Bengio e Yann LeCun, ganhadores do Prêmio Turing em 2018, permitindo análises preditivas altamente sofisticadas e automatizadas.

Ampla utilização de plataformas analíticas avançadas como Hadoop (Dean e Ghemawat, 2004) e Apache Spark (Zaharia et al., 2010) para o processamento distribuído e eficiente de grandes conjuntos de dados.

Atualidade e Futuro:

Emergência da computação quântica como fronteira tecnológica para resolver problemas analíticos complexos, com potencial para revolucionar novamente a análise preditiva e processamento de dados massivos (Nielsen e Chuang, 2010).

Integração cada vez maior de Inteligência Artificial (IA) avançada, como GPT e outras tecnologias de aprendizado profundo, ampliando as capacidades preditivas e analíticas das empresas.

